from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments
from datasets import load_dataset, DatasetDict
import numpy as np
import torch

# 1. Load the Dataset
# Assuming your CSV has a column named "rewritten_prompt" which contains the prompts you want to train on.
# Modify the file path if necessary.
dataset = load_dataset("csv", data_files="adversarial_prompts.csv")

# 2. Preprocessing: Tokenize the Dataset
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


# Define a function to tokenize each sample
def tokenize_function(examples):
    # Tokenize the "rewritten_prompt" field
    return tokenizer(
        examples["rewritten_prompt"],
        truncation=True,
        padding="max_length",
        max_length=128,
    )


# Apply the tokenizer to the dataset
tokenized_dataset = dataset.map(
    tokenize_function, batched=True, remove_columns=["rewritten_prompt"]
)

# 3. Split the Dataset into Training and Validation Sets
# Let's split the dataset into 80% training and 20% validation
train_test_split = tokenized_dataset["train"].train_test_split(test_size=0.2)
dataset_split = DatasetDict(
    {"train": train_test_split["train"], "validation": train_test_split["test"]}
)

# 4. Load Pre-Trained GPT Model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# 5. Define the Training Arguments
training_args = TrainingArguments(
    output_dir="./results",  # Output directory
    overwrite_output_dir=True,  # Overwrite the content of the output directory
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    per_device_train_batch_size=2,  # Adjust based on your hardware
    per_device_eval_batch_size=2,
    num_train_epochs=3,  # You can increase this based on the amount of data
    logging_dir="./logs",  # Directory for storing logs
    save_steps=10_000,  # Save model every 10k steps
    save_total_limit=2,  # Only keep the last 2 models
    load_best_model_at_end=True,  # Load the best model after training ends
    metric_for_best_model="eval_loss",  # Use validation loss as the metric
    report_to="none",  # Disable reporting (optional)
)

# 6. Define Trainer and Train the Model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_split["train"],
    eval_dataset=dataset_split["validation"],
    tokenizer=tokenizer,
)

# 7. Fine-Tune the Model
trainer.train()

# 8. Save the Final Model
model.save_pretrained("./final_model")
tokenizer.save_pretrained("./final_model")

# 9. Evaluation (Optional)
# Evaluate the final model on the validation set
# results = trainer.evaluate()
# print("Evaluation Results:", results)
